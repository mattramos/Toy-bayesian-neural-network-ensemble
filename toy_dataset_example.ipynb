{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "v6OkxcgqlJvz",
    "outputId": "adeabbf5-0b40-467c-9a42-20693a435192"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import baynne as bn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import datetime\n",
    "\n",
    "# Make reproducible\n",
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "The Bayesian neural network ensemble (BayNNE) is a tool which weights individual physical models within an ensemble to genearate more accurate predictions, whilst maintaining a principled prediction of uncertainty.\n",
    "\n",
    "We generate predictions $y$ from a combination of individual model predictions $M_i$ multiplied by weights $\\alpha_i$. We also include a bias term $\\beta$ and aleatoric (observation) noise $\\sigma$. The BayNNE learns the weights, bias and observational uncertainty. The ensemble means that predictions are suitably uncertain for areas where we have no/less observational data.\n",
    "\n",
    "$ y(\\textbf{x},t) = \\sum_{i=1}^n \\alpha_i(\\textbf{x},t)M_i(\\textbf{x},t) + \\beta(\\textbf{x},t) + \\sigma(\\textbf{x},t)$ \n",
    "\n",
    "In this notebook we demonstrate how the BayNNE works with a synthetic dataset which replicates a typical climate model ensemble with surface observations. We make noisey observations, and output from 4 biased models (all synthetic) which are only good at predicting in certain regions. You will see that the BayNNE successfully learns how to ensemble the models to generate the most accurate predictions, whilst learning the observational noise. \n",
    "\n",
    "For more info on the BayNNE check out our paper: LINK\n",
    "\n",
    "### Notes on running the notebook\n",
    "The synthetic problem is set up to be coarse so it runs quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLrqZt_-mJiW"
   },
   "source": [
    "# Create synthetic observations\n",
    "- First describe the spatio temporal grid (latitude, longitude, time, months)\n",
    "- Then create a function for the synthetic observations\n",
    "\n",
    "You'll notice that we've defined the time coordinate both through time, and the month number. We do this because it encourages the BayNNE to learn something about the seasonality of model performance, something which is often the case in real geophysical models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpgIKZiSmDMv"
   },
   "outputs": [],
   "source": [
    "n_years = 20\n",
    "lats = np.arange(-90, 90, 180/15)\n",
    "lons = np.arange(-180,180, 360/12)\n",
    "time = np.arange(0, n_years, 1/12)\n",
    "mons = np.tile(np.arange(1, 13, 1), n_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QPauEdomDFE"
   },
   "outputs": [],
   "source": [
    "def obs_fun(time, lat, lon, month):\n",
    "    f = (0.5 * (((lat/90)  ** 2) + 0.5 * np.sin(2 * np.pi * lon/180)) - 0.2 * np.cos(np.pi * month/6))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqhsBp8ymA0u"
   },
   "outputs": [],
   "source": [
    "obs = np.zeros([len(time), len(lats), len(lons)])\n",
    "for i, t in enumerate(time):\n",
    "    for j, lat in enumerate(lats):\n",
    "        for k, lon in enumerate(lons):\n",
    "            obs[i, j, k] = obs_fun(t, lat, lon, mons[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wRqrux4n5Dj"
   },
   "source": [
    "Now we can make some synthetic model output, which perform well in certain regions and have specified bias. The models also have a small amount of noise added (0.005). To mimic models performing poorly we use Gaussian noise.\n",
    "- Model 1: True from 30N to 90N with a +0.03 bias\n",
    "- Model 2: True from 30S to 30N with no bias\n",
    "- Model 3: True from 30S to 30N with no bias\n",
    "- Model 4: True from 90S to 30S with -0.03 bias\n",
    "\n",
    "We also define that the observations have the following noise:\n",
    "- 0.01 for 30N to 90N \n",
    "- 0.02 for 30S to 30N \n",
    "- 0.03 for 90S to 30S "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYlUOPlgn4gr"
   },
   "outputs": [],
   "source": [
    "idx_30s = int(len(lats) / 3)\n",
    "idx_30n = - idx_30s\n",
    "\n",
    "mdl1 = obs.copy() - 0.03 + np.random.normal(size=[len(time), len(lats), len(lons)]) * 0.005\n",
    "mdl1[:,:idx_30n, :] = np.random.random([len(time), len(lats) - idx_30s, len(lons)]) * 2 - 1\n",
    "mdl2 = obs.copy() + np.random.normal(size=[len(time),len(lats), len(lons)]) * 0.005 \n",
    "mdl2[:,:idx_30s, :] = np.random.random([len(time), idx_30s, len(lons)]) * 2 - 1\n",
    "mdl2[:,idx_30n:, :] = np.random.random([len(time), idx_30s, len(lons)]) * 2 - 1\n",
    "mdl3 = obs.copy() + np.random.normal(size=[len(time), len(lats), len(lons)]) * 0.005\n",
    "mdl3[:,:idx_30s, :] = np.random.random([len(time), idx_30s, len(lons)]) * 2 - 1\n",
    "mdl3[:,idx_30n:, :] = np.random.random([len(time), idx_30s, len(lons)]) * 2 - 1\n",
    "mdl4 = obs.copy() + 0.03 + np.random.normal(size=[len(time), len(lats), len(lons)]) * 0.005\n",
    "mdl4[:,idx_30s:, :] = np.random.random([len(time), len(lats) - idx_30s, len(lons)]) * 2 - 1\n",
    "\n",
    "# Add noise\n",
    "obs[:,idx_30n:, :] = obs[:,idx_30n:, :] + np.random.normal(size=obs[:,idx_30n:, :].shape) * 0.01\n",
    "obs[:,idx_30s:idx_30n, :] = obs[:,idx_30s:idx_30n, :] + np.random.normal(size=obs[:,idx_30s:idx_30n, :].shape) * 0.02\n",
    "obs[:,:idx_30s, :] = obs[:,:idx_30s, :] + np.random.normal(size=obs[:,:idx_30s, :].shape) * 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBuomJbGBV84"
   },
   "source": [
    "### Here we plot the synthetic obs and models for the first month of the first year\n",
    "You can see how the models only perform well in certain regions and that the predictions are nonsense out side of these regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KILkr_K8BVLa"
   },
   "outputs": [],
   "source": [
    "# Set up basemap plots\n",
    "m=Basemap(lat_0=0, lon_0=0, projection='robin', resolution='c')\n",
    "lon = np.arange(-180 ,181,360/len(lons))\n",
    "lat = np.arange(-90 , 91, 180/len(lats))\n",
    "lons_plot, lats_plot = np.meshgrid(lon, lat)\n",
    "x_plot, y_plot = m(lons_plot, lats_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "id": "ggoPgGCWCj2C",
    "outputId": "cfbeab1b-c38e-450b-aed5-4ca36b1d0cff"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(11,14))\n",
    "\n",
    "p1 = m.pcolormesh(x_plot, y_plot, mdl1[0], vmin=-1, vmax=1, ax=axes[0][0])\n",
    "axes[0][0].title.set_text('Model 1')\n",
    "m.drawcoastlines(color='white', ax=axes[0][0])\n",
    "\n",
    "p2 = m.pcolormesh(x_plot, y_plot, mdl2[0], vmin=-1, vmax=1, ax=axes[0][1])\n",
    "axes[0][1].title.set_text('Model 2')\n",
    "m.drawcoastlines(color='white', ax=axes[0][1])\n",
    "\n",
    "p3 = m.pcolormesh(x_plot, y_plot, mdl3[0], vmin=-1, vmax=1, ax=axes[1][0])\n",
    "axes[1][0].title.set_text('Model 3')\n",
    "m.drawcoastlines(color='white', ax=axes[1][0])\n",
    "\n",
    "p4 = m.pcolormesh(x_plot, y_plot, mdl4[0], vmin=-1, vmax=1, ax=axes[1][1])\n",
    "axes[1][1].title.set_text('Model 4')\n",
    "m.drawcoastlines(color='white', ax=axes[1][1])\n",
    "\n",
    "p5 = m.pcolormesh(x_plot, y_plot, obs[0], vmin=-1, vmax=1, ax=axes[2][1])\n",
    "axes[2][1].title.set_text('Obs')\n",
    "m.drawcoastlines(color='white', ax=axes[2][1])\n",
    "\n",
    "fig.delaxes(axes[2][0])\n",
    "\n",
    "cb = fig.colorbar(p4, ax=axes.ravel().tolist(), location='bottom')\n",
    "cb.set_label('Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVSb8qVAqbZU"
   },
   "source": [
    "# Preprocessing data\n",
    "We put the models, observations and defining coordinates into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x--oeVanqbIJ"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['mdl1'] = mdl1.ravel()\n",
    "df['mdl2'] = mdl2.ravel()\n",
    "df['mdl3'] = mdl3.ravel()\n",
    "df['mdl4'] = mdl4.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbm0rNyJqj5T"
   },
   "source": [
    "### Coordinate mapping\n",
    "\n",
    "To avoid the issue of longitude being discontinuous (0 degrees is the same as 360 degrees but is numerically discontinuous) we convert latitude and longitude to a cartesian coordinate system (x,y,z) and this ensures a 1:1 mapping of coordinates.\n",
    "\n",
    "Before we do this we have to flatten the coordinate axes. We do this because the neural net is fed one point in time and space at a time, so we need the coordinate axes to describe every one of these points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqThi2cGqbB5"
   },
   "outputs": [],
   "source": [
    "# Make the coordinate axes desriptive of every data point\n",
    "data_len = obs.size\n",
    "lon_data = np.tile(lons, int(data_len / len(lons))).reshape(-1,)\n",
    "lat_data = np.tile(np.repeat(lats, len(lons)), len(time)).reshape(-1,)\n",
    "time_data = np.repeat(time, int(data_len / len(time))).reshape(-1,)\n",
    "mon_data = np.repeat(mons, int(data_len / len(time))).reshape(-1,)\n",
    "\n",
    "# Convert to Cartesian coordinates\n",
    "x = np.cos(lat_data * np.pi / 180)  * np.cos(lon_data * np.pi / 180)\n",
    "y = np.cos(lat_data * np.pi / 180)  * np.sin(lon_data * np.pi / 180)\n",
    "z = np.sin(lat_data * np.pi / 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gbFUGWxq84J"
   },
   "source": [
    "The same is true for the month coordinate (running 1 through 12). So we treat months in 2 dimensions much like converting the twelve hours on a clock face into cartesian coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUUg3QdXq69e"
   },
   "outputs": [],
   "source": [
    "rads = (mon_data / 12 * 360) * (np.pi / 180)\n",
    "x_mon = np.sin(rads)\n",
    "y_mon = np.cos(rads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the input data (min-max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpqaerm1rSEt"
   },
   "source": [
    "The way we have converted the spatial and month coordinates means that they are all between -1 and 1. A min-max scaling of -1 and 1 is important because it means that all the coordinates have equal affect to the weighting algorithm - in this case the neural network. You'll notice that the obs and model output is alerady between -1 and 1 because of the function we chose. If these values were not already scaled we would need to scale them. We therefore also need to min-max scale the temporal coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6orHibGrOp9"
   },
   "outputs": [],
   "source": [
    "time_data_scaled = 2 * (time_data - time_data.min()) / (time_data.max() - time_data.min()) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fx8beWXusZDp"
   },
   "source": [
    "### Coordinate (feature) scaling \n",
    "\n",
    "Though we have scaled everything between -1 and 1, these values can be tweaked to control the complexity (wiggliness) of the weighting function. For example, if we changed the latitude coordinate to be between -3 and 3, then the weighting function will fit a wigglier function with respect to latitude. Below you can see the impact on the model weight as we change the scaling factor for the latitude coordinate. For now we'll just leave everything at 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/wiggliness.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rx7QGA0CsYjh"
   },
   "outputs": [],
   "source": [
    "df['x'] = x * 1\n",
    "df['y'] = y * 1\n",
    "df['z'] = z * 1\n",
    "df['x_mon'] = x_mon * 1\n",
    "df['y_mon'] = y_mon * 1\n",
    "df['time'] = time_data_scaled * 1\n",
    "df['obs'] = obs.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNfy6ZIYt4wE"
   },
   "source": [
    "### Splitting data into train, test and validation\n",
    "\n",
    "The data we've created is 20 years long so to validate the predictions we'll leave the second 10 years out from training. The first ten years will be randomly split 85% and 15% to form the train and test sets respectively.\n",
    "\n",
    "Note: when the train, test and validation sets are put into numpy arrays, we convert them into float32 to boost computational speed in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ljWCEevt4KS"
   },
   "outputs": [],
   "source": [
    "df_train_test = df[:int(10 * len(df)/20)]\n",
    "df_validation = df[int(10 * len(df)/20):]\n",
    "\n",
    "df_shuffled = df_train_test.sample(frac=1, random_state=seed)\n",
    "split_idx = round(len(df_shuffled) * 0.85)\n",
    "df_train = df_shuffled[:split_idx]\n",
    "df_test = df_shuffled[split_idx:]\n",
    "\n",
    "# In sample training\n",
    "X_train = df_train.drop(['obs'],axis=1).values.astype(np.float32)\n",
    "y_train = df_train['obs'].values.reshape(-1,1).astype(np.float32)\n",
    "\n",
    "# The in sample testing - this is not used for training\n",
    "X_test = df_test.drop(['obs'],axis=1).values.astype(np.float32)\n",
    "y_test = df_test['obs'].values.reshape(-1,1).astype(np.float32)\n",
    "\n",
    "# For out of sample validation\n",
    "X_val = df_validation.drop(['obs'],axis=1).values.astype(np.float32)\n",
    "y_val = df_validation['obs'].values.reshape(-1,1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up and initialising the BayNNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emv9kSiRvUHK"
   },
   "source": [
    "We start by defining some priors:\n",
    "- The multi model mean is our prior - this means that the untrained BayNNE should output the multi model mean\n",
    "- Bias(beta term) should be zero with a small standard deviation\n",
    "- Noise. These values are chosen dependent on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZ-aC3SYn4dm"
   },
   "outputs": [],
   "source": [
    "bias_std = 0.01\n",
    "noise_mean = 0.02\n",
    "noise_std = 0.004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AynKBhqmvsMF"
   },
   "source": [
    "### Hyper-parameters\n",
    "\n",
    "These of course can be tuned but we mostly avoided hyperparameter optimization, for reasons discussed in the paper. An ensemble size (of neural nets) of 10 will get decent results but may not fully capture the uncertainty. Here we settle for some basic hyperparam choices for speed of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJRxzwCvvr-t"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_models = 4                      #Â Number of (geo)physical models \n",
    "n_ensembles = 10                    # Number of neural nets in ensemble\n",
    "hidden_size = 50                    # Number of nodes in hidden layers\n",
    "n_epochs = 350                      # Number of epochs to train for\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_data = X_train.shape[0]           # number of obs\n",
    "x_dim = X_train.shape[1]\n",
    "alpha_dim = x_dim - num_models      # Number of coordinates\n",
    "y_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsHPoFE5wfk4"
   },
   "source": [
    "As we said previously, our prior is that all models should be weighted equally i.e. a multi model mean should be the output of the untrained BayNNE. To encode this prior into the BayNNE we determine standard deviations for the networks' weights and bias. This is trial and error to some extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9bZT9BBwfbN"
   },
   "outputs": [],
   "source": [
    "init_stddev_1_w =  np.sqrt(3.0/(alpha_dim))\n",
    "init_stddev_1_b = init_stddev_1_w\n",
    "init_stddev_2_w =  (1.5)/np.sqrt(hidden_size)\n",
    "init_stddev_2_b = init_stddev_2_w\n",
    "init_stddev_3_w = (1.5*bias_std)/np.sqrt(hidden_size)\n",
    "init_stddev_noise_w = (1.0)/np.sqrt(hidden_size)\n",
    "\n",
    "lambda_anchor = 1.0/(np.array([init_stddev_1_w,init_stddev_1_b,init_stddev_2_w,init_stddev_2_b,init_stddev_3_w,init_stddev_noise_w])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgX70YeExhF7"
   },
   "source": [
    "### Initialising the NNs \n",
    "With that all set up we can initialise out the neural networks within the ensemble, where the network weights and biases are drawn from distributions defined by the standard deviations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSZMazxFwfWA"
   },
   "outputs": [],
   "source": [
    "NNs=[]\n",
    "\n",
    "sess = tf.Session()\n",
    "init_weights = []\n",
    "\n",
    "for ens in range(0,n_ensembles):\n",
    "    # These steps initialise the neural networks\n",
    "    NNs.append(bn.NN(x_dim, y_dim, alpha_dim, num_models, n_data, hidden_size, \n",
    "                     init_stddev_1_w, init_stddev_1_b, init_stddev_2_w, init_stddev_2_b, init_stddev_3_w, init_stddev_noise_w,\n",
    "                     learning_rate))\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "    NNs[ens].anchor(sess, lambda_anchor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior check\n",
    "We perform a series of checks to confirm that the untrained BayNNE matches the priors.\n",
    "\n",
    "The most important check is that the presoftmax layer has a standard deviation of about 1 - this ensures that the untrained network is equally likely to predict any combination of model weights. This is shown in the plot below which shows the distribution of model weights ($\\alpha_i$) for three models. You can see when the pre-softmax layer has a standard deviation much below 1 the weights are forced to the centre of the simplex, hence resulting in a multi model mean, but restricitng the possibility of other uneven model combinations. Whereas for a pre-softmax standard deviation > 1 an uneven combination of models is far more likely. A pre-softmax standard deviation of 1 is therefore the goldilocks zone as all model combinations are equally likely.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyqCiHbHyARc"
   },
   "source": [
    "![](images/pre_softmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Bn2aNelUxwSf",
    "outputId": "89792508-bbb6-44b1-e18a-a159e5c653a1"
   },
   "outputs": [],
   "source": [
    "alpha_w = np.array(bn.get_layer2_output(NNs, X_train, sess))\n",
    "print('Presoftmax mean: {}'.format(np.mean(alpha_w.ravel())))\n",
    "print('Presoftmax std: {}'.format(np.mean(np.std(alpha_w, axis=0).ravel())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ou2tIgPRzThD"
   },
   "source": [
    "It also a good idea to check that the model weights (alphas) and model bias (beta) are the values we encoded in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "nJ8XgcBQxeKD",
    "outputId": "378a9532-11ee-4072-f4b7-4328855b78bb"
   },
   "outputs": [],
   "source": [
    "alphas = np.array(bn.get_alphas(NNs, X_train, sess))\n",
    "print('Alpha mean should be: {}'.format(1/num_models))\n",
    "print('Alpha mean is: {}'.format(np.mean(np.array(alphas).ravel())))\n",
    "print('Alpha std should be: {}'.format(np.sqrt((1/(1 + num_models)) * (1/num_models)*(1-(1/num_models)))))\n",
    "print('Alpha std is: {}'.format(np.mean(np.std(np.array(alphas), axis=0).ravel())))\n",
    "print('')\n",
    "\n",
    "### Beta\n",
    "beta = np.array(bn.get_betas(NNs, X_train, sess))\n",
    "print('Beta mean should be: {}'.format(0))\n",
    "print('Beta mean is: {}'.format(np.mean(beta.ravel())))\n",
    "print('Beta std should be: {}'.format(bias_std))\n",
    "print('Beta std is: {}'.format(np.std(beta.ravel())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRyMnq_N41T6"
   },
   "source": [
    "Now we can train. We train each neural network in the ensemble sequentially, shuffling the training data every 50 epochs. In training we record the RMSE, the anchored loss (which is what we are minismising) and the anchoring term, which is a measure of how far away we are from our prior network weights and biases.\n",
    "\n",
    "As it is set up it takes 4mins to train. Of course we've only trained to 300 epochs and we could definitely train more! If you have more time boost the number of epochs or lower the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SXnZwvXj4yJs",
    "outputId": "5c9f78b0-1d00-4b2c-a565-90fbd2b452fc"
   },
   "outputs": [],
   "source": [
    "l_s = []\n",
    "m_s = []\n",
    "a_s = []\n",
    "\n",
    "for ens in range(0,n_ensembles):\n",
    "    ep_ = 0\n",
    "    losses = []\n",
    "    mses = []\n",
    "    anchs = []\n",
    "    eps = []\n",
    "    print('NN:',ens + 1)\n",
    "    while ep_ < n_epochs:\n",
    "        if (ep_ % 50 == 0):\n",
    "            X_train, y_train = shuffle(X_train, y_train, random_state = ep_)\n",
    "\n",
    "        ep_ += 1\n",
    "        for j in range(int(n_data/batch_size)): #minibatch training loop\n",
    "            feed_b = {}\n",
    "            feed_b[NNs[ens].inputs] = X_train[j*batch_size:(j+1)*batch_size, :]\n",
    "            feed_b[NNs[ens].y_target] = y_train[j*batch_size:(j+1)*batch_size, :]\n",
    "            blank = sess.run(NNs[ens].optimizer, feed_dict=feed_b)\n",
    "        if (ep_ % 25) == 0: \n",
    "            feed_b = {}\n",
    "            feed_b[NNs[ens].inputs] = X_train\n",
    "            feed_b[NNs[ens].y_target] = y_train\n",
    "            loss_mse = sess.run(NNs[ens].mse_, feed_dict=feed_b)\n",
    "            loss_anch = sess.run(NNs[ens].loss_, feed_dict=feed_b)\n",
    "            loss_anch_term = sess.run(NNs[ens].loss_anchor, feed_dict=feed_b)\n",
    "            losses.append(loss_anch)\n",
    "            mses.append(loss_mse)\n",
    "            anchs.append(loss_anch_term)\n",
    "            eps.append(ep_)\n",
    "        if (ep_ % 50 == 0):\n",
    "            print('epoch:' + str(ep_) + ' at ' + str(datetime.datetime.now()))\n",
    "            print(', rmse_', np.round(np.sqrt(loss_mse),5), ', loss_anch', np.round(loss_anch,5), ', anch_term', np.round(loss_anch_term,5))\n",
    "    l_s.append(losses)\n",
    "    m_s.append(mses)\n",
    "    a_s.append(anchs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look at training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(eps, np.array(l_s).T)\n",
    "plt.title('anchored loss')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(eps, np.array(m_s).T)\n",
    "plt.title('mean squared error')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(eps, np.array(a_s).T)\n",
    "plt.title('anchoring term')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj9hWEoQ5XQz"
   },
   "source": [
    "# Making and plotting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5JKQn8r4yHB"
   },
   "outputs": [],
   "source": [
    "y_preds_train, y_preds_mu_train, y_preds_std_train, y_preds_std_train_epi, y_preds_noisesq_train = bn.fn_predict_ensemble(NNs,X_train, sess)\n",
    "y_preds_test, y_preds_mu_test, y_preds_std_test, y_preds_std_test_epi, y_preds_noisesq_test = bn.fn_predict_ensemble(NNs,X_test, sess)\n",
    "y_preds_val, y_preds_mu_val, y_preds_std_val, y_preds_std_val_epi, y_preds_noisesq_val = bn.fn_predict_ensemble(NNs,X_val, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIaGc_3p9lRU"
   },
   "source": [
    "A little function for putting the flattened data back into 3 dimensions for easy plotting, and another for assessing how good the uncertainty quantification is (i.e. is it inline with Gaussian errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csJRJf2Y4yEv"
   },
   "outputs": [],
   "source": [
    "def reshape3d(in_array):\n",
    "\n",
    "    lat_len = len(lats)\n",
    "    lon_len = len(lons)\n",
    "    time_len = len(time)\n",
    "\n",
    "    output = np.zeros([time_len, lat_len, lon_len])\n",
    "\n",
    "    for t in range(time_len):\n",
    "        output[t,:,:] = in_array[lat_len * lon_len * (t): lat_len * lon_len * (t+1)].reshape([lat_len, lon_len])\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def report_on_percentiles(y, y_pred, y_std):\n",
    "\n",
    "    n = len(y.ravel())\n",
    "    n1 = np.sum(np.abs(y_pred.ravel() - y.ravel()) <= y_std.ravel() * 1)\n",
    "    n2 = np.sum(np.abs(y_pred.ravel() - y.ravel()) <= y_std.ravel() * 2)\n",
    "    n3 = np.sum(np.abs(y_pred.ravel() - y.ravel()) <= y_std.ravel() * 3)\n",
    "    print('{} within 1 std'.format(100 * n1 / n))\n",
    "    print('{} within 2 std'.format(100 * n2 / n))\n",
    "    print('{} within 3 std'.format(100 * n3 / n))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the BayNNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2kr9Pd79svK"
   },
   "source": [
    "Firstly let's look at the performance of the BayNNE - root mean squared error and negative log likelihood for train, test and validation. The NLL and RMSE of the multi model mean (MMM) are also shown for comparisson as this the oft used ensembling method with geophysical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "07FK6o0k4yCi",
    "outputId": "b3f37779-2914-4eec-a482-1bf71d0af25d"
   },
   "outputs": [],
   "source": [
    "print('Train NLL: {}'.format(np.mean(0.5*((((y_preds_mu_train.ravel() - y_train.ravel())**2)/((y_preds_std_train.ravel()**2)) + np.log(y_preds_std_train.ravel()**2) + np.log(2*np.pi))))))\n",
    "print('Test NLL: {}'.format(np.mean(0.5*((((y_preds_mu_test.ravel() - y_test.ravel())**2)/((y_preds_std_test.ravel()**2)) + np.log(y_preds_std_test.ravel()**2) + np.log(2*np.pi))))))\n",
    "print('Out of sample NLL: {}'.format(np.mean(0.5*((((y_preds_mu_val.ravel() - y_val.ravel())**2)/((y_preds_std_val.ravel()**2)) + np.log(y_preds_std_val.ravel()**2) + np.log(2*np.pi))))))\n",
    "print('MMM NLL: {}'.format(np.mean(0.5*((((np.mean(X_val[:,:4], axis=1) - y_val.ravel())**2)/((np.std(X_val[:,:4], axis=1)**2)) + np.log(np.mean(X_val[:,:4], axis=1)**2) + np.log(2*np.pi))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "qy73uyKb4x-_",
    "outputId": "aadd84ca-a181-4c66-ddc0-d3efb4e9bbe2"
   },
   "outputs": [],
   "source": [
    "print('Train RMSE: {}'.format(np.sqrt(np.mean(np.square(y_preds_mu_train.ravel() - y_train.ravel())))))\n",
    "print('Test RMSE: {}'.format(np.sqrt(np.mean(np.square(y_preds_mu_test.ravel() - y_test.ravel())))))\n",
    "print('Out of sample RMSE: {}'.format(np.sqrt(np.mean(np.square(y_preds_mu_val.ravel() - y_val.ravel())))))\n",
    "print('MMM RMSE: {}'.format(np.sqrt(np.mean(np.square(np.mean(X_val[:,:4], axis=1) - y_val.ravel())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE and NLL look good as they're consistent across the train, test and validation datasets, and are (unsurprisingly) better than a multi model mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nX__BVa-w5d"
   },
   "source": [
    "We can look if the uncertainty quantification is good. We would expect 68%, 95%, 98% of the points to lie within 1, 2, 3 standard deviations respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "iL_-6iZpxeGk",
    "outputId": "7a99280c-bc6c-4dd1-d529-8158ff8468b5"
   },
   "outputs": [],
   "source": [
    "print('For training')\n",
    "report_on_percentiles(y_train, y_preds_mu_train, y_preds_std_train)\n",
    "print('For testing')\n",
    "report_on_percentiles(y_test, y_preds_mu_test, y_preds_std_test)\n",
    "print('For out of sample')\n",
    "report_on_percentiles(y_val, y_preds_mu_val, y_preds_std_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of prediction\n",
    "This shows the prediction from the BayNNE compared to the true observations. The plots show the last month in the observations - so the most out of sample time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11,6))\n",
    "\n",
    "p1 = m.pcolormesh(x_plot, y_plot, y_preds_mu_val.reshape(120,len(lats),len(lons))[-1], vmin=-1, vmax=1, ax=axes[0])\n",
    "axes[0].xaxis.set_ticklabels([])\n",
    "axes[0].title.set_text('Prediction')\n",
    "m.drawcoastlines(color='white', ax=axes[0])\n",
    "\n",
    "p2 = m.pcolormesh(x_plot, y_plot, y_val.reshape(120,len(lats),len(lons))[-1], vmin=-1, vmax=1, ax=axes[1])\n",
    "axes[1].xaxis.set_ticklabels([])\n",
    "axes[1].yaxis.set_ticklabels([])\n",
    "axes[1].title.set_text('Real obs')\n",
    "m.drawcoastlines(color='white', ax=axes[1])\n",
    "\n",
    "cb = fig.colorbar(p4, ax=axes.ravel().tolist(), location='bottom')\n",
    "cb.set_label('Model weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NhBu9-O_cA1"
   },
   "source": [
    "# What are the weights, bias and noise?\n",
    "\n",
    "Has the BayNNE managed to recover the model weights, biases and aleatoric (observational) noise we designed the synthetic data with?  Note: we split in sample (in) and out of sample (validation) seperately - largely because of RAM constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItCHhxAN_b2X"
   },
   "outputs": [],
   "source": [
    "X_in = df_train_test.drop(['obs'],axis=1).values.astype(np.float32)\n",
    "y_in = df_train_test['obs'].values.reshape(-1,1).astype(np.float32)\n",
    "\n",
    "alphas = np.array(bn.get_alphas(NNs, X_in, sess))\n",
    "alpha_in = np.mean(alphas, axis=0)\n",
    "\n",
    "betas = np.array(bn.get_betas(NNs, X_in, sess))\n",
    "beta_in = np.mean(betas, axis=(0,2))\n",
    "\n",
    "aleatoric_noises = bn.get_aleatoric_noise(NNs, X_in, sess)\n",
    "a_n_in = np.mean(np.array(aleatoric_noises), axis=0)\n",
    "\n",
    "alphas = np.array(bn.get_alphas(NNs, X_val, sess))\n",
    "alpha_out = np.mean(alphas, axis=0)\n",
    "\n",
    "betas = np.array(bn.get_betas(NNs, X_val, sess))\n",
    "beta_out = np.mean(betas, axis=(0,2))\n",
    "\n",
    "aleatoric_noises = bn.get_aleatoric_noise(NNs, X_val, sess)\n",
    "a_n_out = np.mean(np.array(aleatoric_noises), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHLLruRPMwZ2"
   },
   "outputs": [],
   "source": [
    "noise = reshape3d(np.hstack([a_n_in, a_n_out]))\n",
    "alpha = np.vstack([alpha_in, alpha_out])\n",
    "beta = np.hstack([beta_in, beta_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "pNJa8_NK_b0J",
    "outputId": "7e5d5847-f182-4944-9ed3-aa8cd24eee53"
   },
   "outputs": [],
   "source": [
    "# Plot model coefficients\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12,8))\n",
    "\n",
    "p1 = m.pcolormesh(x_plot, y_plot, reshape3d(alpha[:, 0])[180], vmin=0, vmax=1, ax=axes[0][0])\n",
    "axes[0][0].xaxis.set_ticklabels([])\n",
    "axes[0][0].title.set_text('Model 1')\n",
    "m.drawcoastlines(color='white', ax=axes[0][0])\n",
    "\n",
    "p2 = m.pcolormesh(x_plot, y_plot, reshape3d(alpha[:, 1])[180], vmin=0, vmax=1, ax=axes[0][1])\n",
    "axes[0][1].xaxis.set_ticklabels([])\n",
    "axes[0][1].yaxis.set_ticklabels([])\n",
    "axes[0][1].title.set_text('Model 2')\n",
    "m.drawcoastlines(color='white', ax=axes[0][1])\n",
    "\n",
    "p3 = m.pcolormesh(x_plot, y_plot, reshape3d(alpha[:, 2])[180], vmin=0, vmax=1, ax=axes[1][0])\n",
    "axes[1][0].title.set_text('Model 3')\n",
    "m.drawcoastlines(color='white', ax=axes[1][0])\n",
    "\n",
    "p4  = m.pcolormesh(x_plot, y_plot, reshape3d(alpha[:, 3])[180], vmin=0, vmax=1, ax=axes[1][1])\n",
    "axes[1][1].yaxis.set_ticklabels([])\n",
    "axes[1][1].title.set_text('Model 4')\n",
    "m.drawcoastlines(color='white', ax=axes[1][1])\n",
    "\n",
    "p5 = m.pcolormesh(x_plot, y_plot, reshape3d(beta)[180], vmin=-0.03, vmax=0.03, ax=axes[0][2], cmap=plt.cm.inferno)\n",
    "axes[0][2].yaxis.set_ticklabels([])\n",
    "axes[0][2].title.set_text('Model bias')\n",
    "m.drawcoastlines(color='white', ax=axes[0][2], )\n",
    "cbar = m.colorbar(p5, ax=axes[0][2])\n",
    "cbar.set_ticks([-0.03,0,0.03])\n",
    "\n",
    "p6 = m.pcolormesh(x_plot, y_plot, noise[180], ax=axes[1][2], vmin=0.01, cmap=plt.cm.inferno)\n",
    "axes[1][2].yaxis.set_ticklabels([])\n",
    "axes[1][2].title.set_text('Aleatoric noise')\n",
    "m.drawcoastlines(color='white', ax=axes[1][2])\n",
    "m.colorbar(p6, ax=axes[1][2])\n",
    "\n",
    "cb = fig.colorbar(p4, ax=axes[:2, :2].ravel().tolist(), location='bottom')\n",
    "cb.set_label('Model weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BayNNE has recoverd the model weights we expected:\n",
    "- Model 1 is good in the north\n",
    "- Model 2 and 3 are equally good over the tropics\n",
    "- Model 4 is good in the south\n",
    "\n",
    "The biases and noise are almost as we created them also. Of course the networks are not fully trained to convergance - the results would improve further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How certain is the network out of sample?\n",
    "This is one of the key features of the BayNNE: that out of sample, or in areas of sparse data, the uncertainty of predictions is suitably increased. In the plot below you can see how the (area averaged) epistemic uncertainty has increases for the out of sample region where we have no data. This is extremely desirable behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, y_preds_std_in_epi, _ = bn.fn_predict_ensemble(NNs,X_in, sess)\n",
    "epi_unc = reshape3d(np.hstack([y_preds_std_in_epi, y_preds_std_val_epi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_epi = np.mean(epi_unc, axis=(1,2))\n",
    "plt.plot(time[:n_years * 12//2], mean_epi[:n_years * 12//2], label='In sample')\n",
    "plt.plot(time[n_years * 12//2:], mean_epi[n_years * 12//2:], label='Out of sample')\n",
    "plt.ylabel('Epistemic uncertainty')\n",
    "plt.xlabel('Years')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the notebook ran us through a simple example of applying the BayNNE to a small ensemble of models. This has applications in filling in missing observations and extending observational records using models. As we've shown, this can be produce very accurate predictions by learning where and when models are more useful.\n",
    "\n",
    "The BayNNE handles uncertainty well, by correctly recovering observational noise, but also in how the epistemic uncertainty increases where data is sparse or non existent. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "toy_dataset",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
